# 1.1 history:

![结构图](https://raw.githubusercontent.com/JDwangmo/deepLearningBook/master/image/Figure-1.7.png  )

- DL其实不是个新兴的事物，最早在1940s就有了，只不过当时不叫DL，在发展的过程中，出现很多不同的名字，主要有三大波（waves）,如图（Figure 1.7）：
    - 控制论（cybernetics）：1940s-1960s，受到生物学习（biological learning）的影响; artificial neural networks (ANNs); 单神经元感知机（perceptron）。
    - 联接主义（connectionsim）：1980s-1990s，出现了反向传播（back propagation，BP）算法，和多层隐含层的神经网络。
    - 深度学习（DL）：起源于2006年。

- 第一阶段：
    - 早期的模型是一些更简单的线性模型，由神经科学的观点所激发(motivated by a neuroscientific perspective),这些模型输入n个输入，然后去找出跟输出y的关系。
    - M-P 神经元模型（McCulloch-Pitts Neuron）: `P15-top`:这是早期的模拟大脑功能的模型，可以识别二分类。生物原理是：在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位;如果某神经元的电位超过了一个“阈值”（threshold）(所以也叫做 “阈值逻辑单元”(threshold logic unit)),那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。这就是最初的也是沿用至今的 M-P 神经元模型。即 These models were designed to take a set of n input values $$x_1,...,x_n$$ and associate them with an output y. These models would learn a set of weights $$w_1,...,w_n$$ and compute their output f(**x,w**) = x1w1 + ··· +xn
    - perceptron：感知机
    - ADALINE（adaptive linear element）：自适应线性单元。
    - `P15-center`: 模型的训练还是 **随机梯度下降（SGD）**，而且对于现今的DL模型，SGD仍然是主要的训练方法。（The training algorithm used to adapt the weights of the ADALINE was a special case of an algorithm called **stochastic gradient descent**. Slightly modified versions of the stochastic gradient descent algorithm remain the dominant training algorithms for deep learning models today.） 


$$x_1$$
