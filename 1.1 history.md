# 1.1 history:

![结构图](https://raw.githubusercontent.com/JDwangmo/deepLearningBook/master/image/Figure-1.7.png  )

- DL其实不是个新兴的事物，最早在1940s就有了，只不过当时不叫DL，在发展的过程中，出现很多不同的名字，主要有三大波（waves）,如图（Figure 1.7）：
    - 控制论（cybernetics）：1940s-1960s，受到生物学习（biological learning）的影响; artificial neural networks (ANNs); 单神经元感知机（perceptron）。
    - 联接主义（connectionsim）：1980s-1990s，出现了反向传播（back propagation，BP）算法，和多层隐含层的神经网络。
    - 深度学习（DL）：起源于2006年。

- 第一阶段：
    - 早期的模型是一些更简单的线性模型，由神经科学的观点所激发(motivated by a neuroscientific perspective),这些模型输入n个输入，然后去找出跟输出y的关系。
    - M-P 神经元模型（McCulloch-Pitts Neuron）: `P15-top`:这是早期的模拟大脑功能的模型，可以识别二分类。生物原理是：在生物神经网络中，每个神经元与其他神经元相连，当它兴奋时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位;如果某神经元的电位超过了一个“阈值”（threshold）(所以也叫做 “阈值逻辑单元”(threshold logic unit)),那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。这就是最初的也是沿用至今的 M-P 神经元模型。即 These models were designed to take a set of n input values $$x_1,...,x_n$$ and associate them with an output y. These models would learn a set of weights $$w_1,...,w_n$$ and compute their output f(**x,w**) = $$x_1w_1 + ··· +x_n$$。最后通过判断 f(**x,w**) 是否大于某个阈值来预测positve/negative的二分类问题。
    - perceptron：感知机
    - ADALINE（adaptive linear element）：自适应线性单元。
    - `P15-center`: 模型的训练还是 **随机梯度下降（SGD）**，而且对于现今的DL模型，SGD仍然是主要的训练方法。（The training algorithm used to adapt the weights of the ADALINE was a special case of an algorithm called **stochastic gradient descent**. Slightly modified versions of the stochastic gradient descent algorithm remain the dominant training algorithms for deep learning models today.） 
    - 基于 perceptron 和 ADALINE的 f(**x,w**) 一般都称为 线性模型（linear models）。这些线性模型有很多缺点（limitations）：比如线性模型不能解决非线性切分的问题，比如著名的**XOR亦或问题**。而这也导致了第一阶段的低点（Critics who observed these flaws in linear models caused a backlash against biologically inspired learning in general）。

- 第二阶段（connectionsim）：
    - `P17-center`:这个阶段的核心思想是：通过大量的计算单元（神经元）的网络协作，可以达到智能的行为（ The central idea in connectionism is that a large number of simple computational units can achieve intelligent behavior when networked together. This insight applies equally to neurons in biological nervous systems and to hidden units in computational models）。
    - **distributed representation：**
    - **back propagation：**
    - `P18-bottom`： 这个阶段，由于计算代价和硬件（hardware capabilities）的问题，这个时候的DL遇到了训练的困难。At this point in time, deep networks were generally believed to be very difficult to train. We now know that algorithms that have existed since the 1980s work quite well, but this was not apparent circa 2006. The issue is perhaps simply that these algorithms were too computationally costly to allow much experimentation with the hardware available at the time. 

- 当今DL（modern deep learning）：
    - 虽然神经科学仍然被认为是DL的一个重要启示，但已经不再是主要的指导方向（it is no longer the predominant guide for the field ）了。主要原因就是我们关于大脑的了解太少了（we are far from understanding even some of the most simple and well-studied parts of the brain）。
    - 但是，神经科学家还是给了一个重要的启示：哺乳动物可能只是使用一个算法去解决大多数的不同任务（. This suggests that much of the mammalian brain** might use a single algorithm to solve most of the different tasks** that the brain solves）。这也给了我们一个理由**去希望使用一个单一的 DL 算法取解决很多不同的任务（hope that a single deep learning algorithm can solve many different tasks**）。在这个假设（hypothesis）之前，机器学习的研究非常分散/破碎（fragmented），不同社区（communities）的研究者研究不同的领域的东西，有NLP、vision、motion planning和speech recognition 等。**虽然现在这些应用仍然是分离的，但对DL社区的研究人员通常可以同时研究很多或更多领域的应用**（it is common for deep learning research groups to study many or even all of these application areas simultaneousl）
    - 这个阶段，已经可以训练更深的网络This wave of neural networks research popularized the use of the term “deep learning” to emphasize that researchers were now able to **train deeper neural networks** than had been possible before, and to focus attention on the theoretical importance of depth (Bengio and LeCun， 2007; Delalleau and Bengio，2011; Pascanu et al.，2014a; Montufar et al.,2014 )
    - new unsupervised learning techniques and the ability of deep models to generalize well from small datasets.
    - **Increasing Dataset Sizes**：`P19-center，1.2.2` 这个时期，DL能这么辉煌的一个重要原因就是：训练数据的不断积累增大，我们可以提供给DL模型更多的训练例子，现在动不动一个数据集就是上千万。“Big Data”让ML更加容易。**一个大概的拇指规则（a rough rule of thumb）指出一个监督的DL算法在提供了每个类别5000个标签数据的时候可以取得不错的结果，而至少需要1亿的训练例子可以达到人类的性能。（a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category, and will match or exceed human performance when trained with a dataset containing at least _10 million labeled examples_.）**。通常有标签的数据集没有那么多，所以通常会结合无监督或者半监督的学习方法,使得利用更大量的无标签数据。（take advantage of large quantities of unlabeled examples, with unsupervised or semi-surpervised learning）。
    - **Increasing Model Sizes**: `P20-center，1.2.3`  不断变大的模型网络。另外一个神经网络在当今可以如此成功的原因就是我们现在硬件配置等计算资源（computational resources）支持我们可以运行更大的网络架构的模型（larger models）。我们前面也提到第二阶段（联接主义）的一个核心思想就是动物可以通过很多神经元的协作，而达到智能，而单个或者少量集合的神经元都没有特别有效（One of the main insights of connectionism is that animals become intelligent when many of their neurons work together. An individual neuron or small collection of neurons is not particularly useful）。模型的变大主要有两部分的扩充：一、神经元之间的连接数，[原文的Figure 1.10](https://raw.githubusercontent.com/JDwangmo/deepLearningBook/master/book/www.deeplearningbook.org_contents_intro.pdf)展示了多种神经网络模型的连接数的变化和比较情况。二、神经元的数量，[原文的Figure 1.10](https://raw.githubusercontent.com/JDwangmo/deepLearningBook/master/book/www.deeplearningbook.org_contents_intro.pdf)展示了多种神经网络模型的神经元数的比较情况，人类大概有$$10^(11)$$个神经元。
